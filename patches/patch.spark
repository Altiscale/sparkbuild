diff -uNr spark/bin/compute-classpath.sh spark/bin/compute-classpath.sh
--- vcc-spark/bin/compute-classpath.sh	2014-03-27 23:16:37.000000000 -0700
+++ vcc-spark/bin/compute-classpath.sh	2014-03-27 23:21:47.000000000 -0700
@@ -64,6 +64,11 @@
   CLASSPATH="$CLASSPATH:$FWDIR/streaming/target/scala-$SCALA_VERSION/test-classes"
 fi
 
+if [ -d "$HADOOP_HOME/share/hadoop/mapreduce/lib/" ] ; then
+  HADOOP_MR_LZO_JAR=`ls $HADOOP_HOME/share/hadoop/mapreduce/lib/*lzo*.jar`
+  CLASSPATH=$CLASSPATH:$HADOOP_MR_LZO_JAR
+fi
+
 # Add hadoop conf dir if given -- otherwise FileSystem.*, etc fail !
 # Note, this assumes that there is either a HADOOP_CONF_DIR or YARN_CONF_DIR which hosts
 # the configurtion files.
diff -uNr spark/bin/run-example spark/bin/run-example
--- vcc-spark/bin/run-example	2014-03-27 23:16:37.000000000 -0700
+++ vcc-spark/bin/run-example	2014-03-27 23:22:19.000000000 -0700
@@ -76,6 +76,8 @@
   fi
 fi
 
+SPARK_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$SPARK_LIBRARY_PATH
+
 # Set JAVA_OPTS to be able to load native libraries and to set heap size
 JAVA_OPTS="$SPARK_JAVA_OPTS"
 JAVA_OPTS="$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH"
diff -uNr spark/bin/spark-class spark/bin/spark-class
--- vcc-spark/bin/spark-class	2014-03-27 23:16:37.000000000 -0700
+++ vcc-spark/bin/spark-class	2014-03-27 23:21:47.000000000 -0700
@@ -87,6 +87,8 @@
 SPARK_MEM=${SPARK_MEM:-512m}
 export SPARK_MEM
 
+SPARK_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$SPARK_LIBRARY_PATH
+
 # Set JAVA_OPTS to be able to load native libraries and to set heap size
 JAVA_OPTS="$OUR_JAVA_OPTS"
 JAVA_OPTS="$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH"
diff -uNr spark/pom.xml spark/pom.xml
--- vcc-spark/pom.xml	2014-03-27 23:16:37.000000000 -0700
+++ vcc-spark/pom.xml	2014-03-27 23:21:47.000000000 -0700
@@ -374,12 +374,6 @@
         <version>3.1</version>
         <scope>test</scope>
       </dependency>
-     <dependency>
-        <groupId>org.mockito</groupId>
-        <artifactId>mockito-all</artifactId>
-        <version>1.8.5</version>
-        <scope>test</scope>
-      </dependency>
       <dependency>
         <groupId>org.scalacheck</groupId>
         <artifactId>scalacheck_${scala.binary.version}</artifactId>
@@ -393,6 +387,11 @@
         <scope>test</scope>
       </dependency>
       <dependency>
+        <groupId>junit</groupId>
+        <artifactId>junit</artifactId>
+        <version>4.11</version>
+      </dependency> 
+      <dependency>
         <groupId>org.apache.zookeeper</groupId>
         <artifactId>zookeeper</artifactId>
         <version>3.4.5</version>
diff -uNr spark/project/SparkBuild.scala spark/project/SparkBuild.scala
--- vcc-spark/project/SparkBuild.scala	2014-03-27 23:16:37.000000000 -0700
+++ vcc-spark/project/SparkBuild.scala	2014-03-27 23:21:47.000000000 -0700
@@ -28,13 +28,13 @@
   // Hadoop version to build against. For example, "1.0.4" for Apache releases, or
   // "2.0.0-mr1-cdh4.2.0" for Cloudera Hadoop. Note that these variables can be set
   // through the environment variables SPARK_HADOOP_VERSION and SPARK_YARN.
-  val DEFAULT_HADOOP_VERSION = "1.0.4"
+  val DEFAULT_HADOOP_VERSION = "2.2.0"
 
   // Whether the Hadoop version to build against is 2.2.x, or a variant of it. This can be set
   // through the SPARK_IS_NEW_HADOOP environment variable.
   val DEFAULT_IS_NEW_HADOOP = false
 
-  val DEFAULT_YARN = false
+  val DEFAULT_YARN = true
 
   // HBase version; set as appropriate.
   val HBASE_VERSION = "0.94.6"
