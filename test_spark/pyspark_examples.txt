# Run pyspark with the following command

cd $SPARK_HOME

spark_event_log_dir=$(grep 'spark.eventLog.dir' /etc/spark/spark-defaults.conf | tr -s ' ' '\t' | cut -f2)

hdfs dfs -put /opt/spark/README.md /tmp/

./bin/pyspark --master yarn --deploy-mode client --queue research --driver-memory 512M --conf spark.eventLog.dir=${spark_event_log_dir}/$USER

# The following statements are sample statements to run pyspark examples
file=sc.textFile("hdfs:///tmp/README.md")
errors = file.filter(lambda line: "scala" in line)
# Count all the errors
errors.count()
# Count errors mentioning MySQL
errors.filter(lambda line: "MySQL" in line).count()
# Fetch the MySQL errors as an array of strings
errors.filter(lambda line: "MySQL" in line).collect()
