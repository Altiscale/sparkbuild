import org.apache.spark.SparkContext
import org.apache.spark.sql.hive._

// sc.setLocalProperty("javax.jdo.option.ConnectionUserName","alti-test-01")
// sc.setLocalProperty("javax.jdo.option.ConnectionPassword","dummypassword123")
// sc.setLocalProperty("javax.jdo.option.ConnectionURL","jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true")

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

// hiveContext.setConf("javax.jdo.option.ConnectionUserName","alti-test-01")
// hiveContext.setConf("javax.jdo.option.ConnectionPassword","dummypassword123")
// hiveContext.setConf("javax.jdo.option.ConnectionURL","jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true")

import hiveContext._

// Create table and clean up data
hiveContext.hql("CREATE TABLE IF NOT EXISTS spark_hive_test_table (key INT, value STRING)")

// load data
hiveContext.hql("LOAD DATA LOCAL INPATH '/opt/spark/examples/src/main/resources/kv1.txt' INTO TABLE spark_hive_test_table")

// Queries are expressed in HiveQL
hiveContext.hql("FROM spark_hive_test_table SELECT key, value").collect().foreach(println)

exit

